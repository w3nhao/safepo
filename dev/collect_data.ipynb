{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import imageio\n",
    "import os\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "def collect_video(init_obs, env, policy):\n",
    "    images = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    episode_return = 0\n",
    "    done = False\n",
    "    obs = init_obs[0]\n",
    "    image = env.render()\n",
    "    images += [image]\n",
    "    \n",
    "    dd = 10 ### collect a few more steps after done\n",
    "    i = 0\n",
    "    while dd:\n",
    "        action = policy.get_action(obs)\n",
    "        try:\n",
    "            obs, reward, truncation, termination, info = env.step(action)\n",
    "            done = info['success']\n",
    "            dd -= done\n",
    "            episode_return += reward\n",
    "            i = i+1\n",
    "            actions += [action]\n",
    "            rewards += [reward]\n",
    "            #print(i, done, dd)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "        if dd != 10 and not done:\n",
    "            break\n",
    "        image = env.render()\n",
    "        images += [image]\n",
    "        \n",
    "    return images, actions, rewards, episode_return\n",
    "\n",
    "def sample_n_frames(frames, n):\n",
    "    new_vid_ind = [int(i*len(frames)/(n-1)) for i in range(n-1)] + [len(frames)-1]\n",
    "    return np.array([frames[i] for i in new_vid_ind])\n",
    "\n",
    "def save_frame(path, frame):\n",
    "    imageio.imwrite(path, frame)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaworld.envs import ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE as env_dict\n",
    "import metaworld.policies as policies\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import imageio\n",
    "import os\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    collection_config = {\n",
    "        \"demos_per_camera\": 25,\n",
    "        \"output_path\": \"./collected_data\",\n",
    "        \"camera_names\": ['corner', 'corner2', 'corner3'], ### possible values: \"corner3, corner, corner2, topview, behindGripper\", None(for random)\n",
    "        \"discard_ratio\": 0.0, ### discard the last {ratio} of the collected videos (preventing failed episodes)\n",
    "        \"render_mode\": \"rgb_array\"\n",
    "    }\n",
    "\n",
    "    # included_tasks = [\"door-open\"]# [\"drawer-open\", \"door-close\", \"basketball\", \"shelf-place\", \"button-press\", \"button-press-topdown\", \"faucet-close\", \"faucet-open\", \"handle-press\", \"hammer\", \"assembly\"]\n",
    "    # included_tasks = [t + \"-v2-goal-observable\" for t in included_tasks]\n",
    "    # #included_tasks = ['assembly-v2-goal-observable', 'basketball-v2-goal-observable', 'bin-picking-v2-goal-observable', 'box-close-v2-goal-observable', 'button-press-topdown-v2-goal-observable', 'button-press-topdown-wall-v2-goal-observable', 'button-press-v2-goal-observable', 'button-press-wall-v2-goal-observable', 'coffee-button-v2-goal-observable', 'coffee-pull-v2-goal-observable', 'coffee-push-v2-goal-observable', 'dial-turn-v2-goal-observable', 'disassemble-v2-goal-observable', 'door-close-v2-goal-observable', 'door-lock-v2-goal-observable', 'door-open-v2-goal-observable', 'door-unlock-v2-goal-observable', 'hand-insert-v2-goal-observable', 'drawer-close-v2-goal-observable', 'drawer-open-v2-goal-observable', 'faucet-open-v2-goal-observable', 'faucet-close-v2-goal-observable', 'hammer-v2-goal-observable', 'handle-press-side-v2-goal-observable', 'handle-press-v2-goal-observable', 'handle-pull-side-v2-goal-observable', 'handle-pull-v2-goal-observable', 'lever-pull-v2-goal-observable', 'pick-place-wall-v2-goal-observable', 'pick-out-of-hole-v2-goal-observable', 'reach-v2-goal-observable', 'push-back-v2-goal-observable', 'push-v2-goal-observable', 'pick-place-v2-goal-observable', 'plate-slide-v2-goal-observable', 'plate-slide-side-v2-goal-observable', 'plate-slide-back-v2-goal-observable', 'plate-slide-back-side-v2-goal-observable', 'peg-unplug-side-v2-goal-observable', 'soccer-v2-goal-observable', 'stick-push-v2-goal-observable', 'stick-pull-v2-goal-observable', 'push-wall-v2-goal-observable', 'reach-wall-v2-goal-observable', 'shelf-place-v2-goal-observable', 'sweep-into-v2-goal-observable', 'sweep-v2-goal-observable', 'window-open-v2-goal-observable', 'window-close-v2-goal-observable']\n",
    "    \n",
    "    # def get_policy(env_name):\n",
    "    #     name = \"\".join(\" \".join(env_name.split('-')[:-3]).title().split(\" \"))\n",
    "    #     policy_name = \"Sawyer\" + name + \"V2Policy\"\n",
    "    #     try:\n",
    "    #         policy = getattr(policies, policy_name)()\n",
    "    #     except:\n",
    "    #         policy = None\n",
    "    #     return policy\n",
    "\n",
    "\n",
    "    ps = {}\n",
    "    for env_name in env_dict.keys():\n",
    "        policy = get_policy(env_name)\n",
    "        if policy is None:\n",
    "            print(\"Policy not found:\", env_name)\n",
    "        else:\n",
    "            ps[env_name] = policy\n",
    "\n",
    "    out_path = collection_config[\"output_path\"]\n",
    "\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    for camera in collection_config[\"camera_names\"]:\n",
    "        demos = []\n",
    "        action_seqs = []\n",
    "        raw_lengths = []\n",
    "        rewards = []\n",
    "        for seed in tqdm(range(42, 42+ceil(collection_config[\"demos_per_camera\"] * (1+collection_config[\"discard_ratio\"])))):\n",
    "            env = env_dict[task](render_mode=collection_config[\"render_mode\"], camera_name=camera, seed=seed)\n",
    "            obs = env.reset()\n",
    "            images, action_seq, reward, _ = collect_video(obs, env, polcy)\n",
    "            # assert len(images) == len(action_seq) + 1 or len(images) == 502\n",
    "            raw_lengths += [len(images)]\n",
    "            demos += [images]\n",
    "            action_seqs += [action_seq]\n",
    "            rewards += [reward]\n",
    "        top_k_ind = np.argsort(raw_lengths)[:collection_config[\"demos_per_camera\"]]\n",
    "        demos = [demos[i] for i in top_k_ind]\n",
    "        raw_lengths = [raw_lengths[i] for i in top_k_ind]\n",
    "        action_seqs = [action_seqs[i] for i in top_k_ind]\n",
    "        rewards = [rewards[i] for i in top_k_ind]\n",
    "        print(f\"vid length bounds: {raw_lengths[0]} ~ {raw_lengths[-1]}\")\n",
    "        \n",
    "        ### save the collected demos\n",
    "        out_dir = os.path.join(out_path, \"-\".join(task.split('-')[:-3]))\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        for i, demo in enumerate(demos):\n",
    "            demo_dir = os.path.join(out_dir, f\"{camera}/{i:03d}\")\n",
    "            os.makedirs(demo_dir, exist_ok=True)\n",
    "            with mp.Pool(10) as p:\n",
    "                p.starmap(save_frame, [(os.path.join(demo_dir, f\"{j:02d}.png\"), frame) for j, frame in enumerate(demo)])\n",
    "            with open(f\"{demo_dir}/action.pkl\", \"wb\") as f:\n",
    "                pickle.dump(action_seqs[i], f)\n",
    "            with open(f\"{demo_dir}/rewards.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(rewards[i], f)\n",
    "        \n",
    "            \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 OmniSafeAI Team. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "from safepo.common.env import make_sa_mujoco_env\n",
    "from safepo.common.model import ActorVCritic\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "\n",
    "def to_tensor(x, dtype=torch.float32):\n",
    "    return torch.as_tensor(x, dtype=dtype)\n",
    "\n",
    "# eval_done = False\n",
    "# eval_obs, _ = eval_env.reset()\n",
    "# eval_obs = torch.as_tensor(eval_obs, dtype=torch.float32)\n",
    "# eval_rew, eval_cost, eval_len = 0.0, 0.0, 0.0\n",
    "# while not eval_done:\n",
    "#     with torch.no_grad():\n",
    "#         act, _, _, _ = model.step(\n",
    "#             eval_obs, deterministic=True\n",
    "#         )\n",
    "#     eval_obs, reward, cost, terminated, truncated, info = eval_env.step(\n",
    "#         act.detach().squeeze().cpu().numpy()\n",
    "#     )\n",
    "#     eval_obs = torch.as_tensor(\n",
    "#         eval_obs, dtype=torch.float32\n",
    "#     )\n",
    "#     eval_rew += reward[0]\n",
    "#     eval_cost += cost[0]\n",
    "#     eval_len += 1\n",
    "#     eval_done = terminated[0] or truncated[0]\n",
    "\n",
    "def collect_video(obs, env, model, camera, height, width):\n",
    "    images = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    costs = []\n",
    "    episode_return = 0\n",
    "    done = False\n",
    "    image = env.render(camera_name=camera, height=height, width=width)\n",
    "    images += [image]\n",
    "    \n",
    "    done = False\n",
    "    while done:\n",
    "        with torch.no_grad():\n",
    "            action, _, _, _ = model.step(\n",
    "                to_tensor(obs), deterministic=True\n",
    "            )\n",
    "        try:\n",
    "            obs, reward, cost, terminated, truncated, info = env.step(\n",
    "                    act.detach().squeeze().cpu().numpy()\n",
    "                )\n",
    "            done = terminated[0] or truncated[0]\n",
    "            episode_return += reward\n",
    "            actions += [action]\n",
    "            rewards += [reward]\n",
    "            costs += [cost]\n",
    "            #print(i, done, dd)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "        image = env.render(camera_name=camera, height=height, width=width)\n",
    "        images += [image]\n",
    "        \n",
    "    return obs, images, actions, rewards, costs, episode_return\n",
    "\n",
    "\n",
    "def sample_n_frames(frames, n):\n",
    "    new_vid_ind = [int(i*len(frames)/(n-1)) for i in range(n-1)] + [len(frames)-1]\n",
    "    return np.array([frames[i] for i in new_vid_ind])\n",
    "\n",
    "\n",
    "def save_frame(path, frame):\n",
    "    imageio.imwrite(path, frame)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    collection_config = {\n",
    "        \"camera_names\": ['vision', 'track', 'fixednear', 'fixedfar'], ### possible values: \"corner3, corner, corner2, topview, behindGripper\", None(for random)\n",
    "        \"discard_ratio\": 0.0, ### discard the last {ratio} of the collected videos (preventing failed episodes)\n",
    "        \"height\": 128,\n",
    "        \"width\": 128,\n",
    "    }\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--exp_dir\", type=str, default='', help=\"the directory of the evaluation\")\n",
    "    parser.add_argument(\"--episodes_per_camera\", type=int, default=25, help=\"the number of episodes to evaluate\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    env_path = args.exp_dir\n",
    "    episodes_per_cam = args.episodes_per_camera\n",
    "    save_dir = env_path.replace('runs', 'collected_data')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # torch.set_num_threads(4)\n",
    "    \n",
    "    config_path = env_path + '/config.json'\n",
    "    config = json.load(open(config_path, 'r'))\n",
    "\n",
    "    env_id = config['task'] if 'task' in config.keys() else config['env_name']\n",
    "    env_norms = os.listdir(env_path)\n",
    "    env_norms = [env_norm for env_norm in env_norms if env_norm.endswith('.pkl')]\n",
    "    final_norm_name = sorted(env_norms)[-1]\n",
    "\n",
    "    model_dir = env_path + '/torch_save'\n",
    "    models = os.listdir(model_dir)\n",
    "    models = [model for model in models if model.endswith('.pt')]\n",
    "    final_model_name = sorted(models)[-1]\n",
    "\n",
    "    model_path = model_dir + '/' + final_model_name\n",
    "    norm_path = env_path + '/' + final_norm_name\n",
    "\n",
    "    eval_env, obs_space, act_space = make_sa_mujoco_env(\n",
    "        num_envs=config['num_envs'], env_id=env_id, seed=None\n",
    "    )\n",
    "\n",
    "    model = ActorVCritic(\n",
    "            obs_dim=obs_space.shape[0],\n",
    "            act_dim=act_space.shape[0],\n",
    "            hidden_sizes=config['hidden_sizes'],\n",
    "        )\n",
    "    model.actor.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    if os.path.exists(norm_path):\n",
    "        norm = joblib.load(open(norm_path, 'rb'))['Normalizer']\n",
    "        eval_env.obs_rms = norm\n",
    "    \n",
    "    for camera in collection_config[\"camera_names\"]:\n",
    "        demos = []\n",
    "        action_seqs = []\n",
    "        raw_lengths = []\n",
    "        rewards = []\n",
    "        costs = []\n",
    "        for seed in tqdm(range(42, episodes_per_cam)):\n",
    "            obs, _ = env.reset()\n",
    "            images, action_seq, reward, cost _ = collect_video(\n",
    "                obs, env, model, camera, \n",
    "                height=collection_config[\"height\"], width=collection_config[\"width\"]\n",
    "            )\n",
    "            # assert len(images) == len(action_seq) + 1 or len(images) == 502\n",
    "            raw_lengths += [len(images)]\n",
    "            demos += [images]\n",
    "            action_seqs += [action_seq]\n",
    "            rewards += [reward]\n",
    "            costs += [cost]\n",
    "            \n",
    "        top_k_ind = np.argsort(raw_lengths)[:collection_config[\"demos_per_camera\"]]\n",
    "        demos = [demos[i] for i in top_k_ind]\n",
    "        raw_lengths = [raw_lengths[i] for i in top_k_ind]\n",
    "        action_seqs = [action_seqs[i] for i in top_k_ind]\n",
    "        rewards = [rewards[i] for i in top_k_ind]\n",
    "        costs = [costs[i] for i in top_k_ind]\n",
    "        print(f\"vid length bounds: {raw_lengths[0]} ~ {raw_lengths[-1]}\")\n",
    "        \n",
    "        ### save the collected demos\n",
    "        import pdb; pdb.set_trace()\n",
    "        out_dir = os.path.join(out_path, \"-\".join(task.split('-')[:-3]))\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        for i, demo in enumerate(demos):\n",
    "            demo_dir = os.path.join(out_dir, f\"{camera}/{i:03d}\")\n",
    "            os.makedirs(demo_dir, exist_ok=True)\n",
    "            with mp.Pool(10) as p:\n",
    "                p.starmap(save_frame, [(os.path.join(demo_dir, f\"{j:02d}.png\"), frame) for j, frame in enumerate(demo)])\n",
    "            with open(f\"{demo_dir}/action.pkl\", \"wb\") as f:\n",
    "                pickle.dump(action_seqs[i], f)\n",
    "            with open(f\"{demo_dir}/rewards.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(rewards[i], f)\n",
    "            with open(f\"{demo_dir}/costs.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(costs[i, f])\n",
    "            \n",
    "\n",
    "\n",
    "    for seed in tqdm(range(42, episodes_per_cam)):\n",
    "\n",
    "        eval_done = False\n",
    "        eval_obs, _ = eval_env.reset()\n",
    "        eval_obs = torch.as_tensor(eval_obs, dtype=torch.float32)\n",
    "        eval_rew, eval_cost, eval_len = 0.0, 0.0, 0.0\n",
    "        while not eval_done:\n",
    "            with torch.no_grad():\n",
    "                act, _, _, _ = model.step(\n",
    "                    eval_obs, deterministic=True\n",
    "                )\n",
    "            eval_obs, reward, cost, terminated, truncated, info = eval_env.step(\n",
    "                act.detach().squeeze().cpu().numpy()\n",
    "            )\n",
    "            eval_obs = torch.as_tensor(\n",
    "                eval_obs, dtype=torch.float32\n",
    "            )\n",
    "            eval_rew += reward[0]\n",
    "            eval_cost += cost[0]\n",
    "            eval_len += 1\n",
    "            eval_done = terminated[0] or truncated[0]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files:  149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_72761/1413090072.py:23: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img = imageio.imread(vision + \"0\" + str(i) + \".png\")\n",
      "/tmp/ipykernel_72761/1413090072.py:25: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img = imageio.imread(vision + str(i) + \".png\")\n",
      "/tmp/ipykernel_72761/1413090072.py:46: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img = imageio.imread(track + \"0\" + str(i) + \".png\")\n",
      "/tmp/ipykernel_72761/1413090072.py:48: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img = imageio.imread(track + str(i) + \".png\")\n",
      "/tmp/ipykernel_72761/1413090072.py:72: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img = imageio.imread(fixednear + \"0\" + str(i) + \".png\")\n",
      "/tmp/ipykernel_72761/1413090072.py:74: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img = imageio.imread(fixednear + str(i) + \".png\")\n"
     ]
    }
   ],
   "source": [
    "# cat all png to gif\n",
    "\n",
    "vision = \"/data/wenhao/20241027_humanoid/Safe-Policy-Optimization/safepo/collected_data/single_agent_exp/SafetyHumanoidVelocity-v1/ppo_lag/seed-000-2024-11-03-01-43-36/vision/004/\"\n",
    "track = \"/data/wenhao/20241027_humanoid/Safe-Policy-Optimization/safepo/collected_data/single_agent_exp/SafetyHumanoidVelocity-v1/ppo_lag/seed-000-2024-11-03-01-43-36/track/004/\"\n",
    "fixednear = \"/data/wenhao/20241027_humanoid/Safe-Policy-Optimization/safepo/collected_data/single_agent_exp/SafetyHumanoidVelocity-v1/ppo_lag/seed-000-2024-11-03-01-43-36/fixednear/004/\"\n",
    "\n",
    "import imageio\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "images = []\n",
    "\n",
    "# get all files in the replay folder\n",
    "files = os.listdir(vision)\n",
    "files = [f for f in files if f.endswith(\".png\")]\n",
    "\n",
    "# get the number of files\n",
    "print(\"Number of files: \", len(files))\n",
    "\n",
    "for i in range(len(files)):\n",
    "    # Load the image\n",
    "    if i < 10:\n",
    "        img = imageio.imread(vision + \"0\" + str(i) + \".png\")\n",
    "    else:\n",
    "        img = imageio.imread(vision + str(i) + \".png\")\n",
    "    \n",
    "    # Rotate the image by 180 degrees\n",
    "    # img_rotated = np.rot90(img, 2)  # Rotates 180 degrees (90 degrees twice)\n",
    "\n",
    "    # # Append the rotated image to the list\n",
    "    # images.append(img_rotated)\n",
    "    images.append(img)\n",
    "\n",
    "imageio.mimsave('/data/wenhao/20241027_humanoid/Safe-Policy-Optimization/safepo/collected_data/single_agent_exp/SafetyHumanoidVelocity-v1/ppo_lag/seed-000-2024-11-03-01-43-36/vision/vision004.gif', images)\n",
    "\n",
    "\n",
    "images = []\n",
    "\n",
    "# get all files in the replay folder\n",
    "files = os.listdir(track)\n",
    "files = [f for f in files if f.endswith(\".png\")]\n",
    "\n",
    "for i in range(len(files)):\n",
    "    # Load the image\n",
    "    if i < 10:\n",
    "        img = imageio.imread(track + \"0\" + str(i) + \".png\")\n",
    "    else:\n",
    "        img = imageio.imread(track + str(i) + \".png\")\n",
    "    \n",
    "    # # Rotate the image by 180 degrees\n",
    "    # img_rotated = np.rot90(img, 2)  # Rotates 180 degrees (90 degrees twice)\n",
    "\n",
    "    # # Append the rotated image to the list\n",
    "    # images.append(img_rotated)\n",
    "    \n",
    "    images.append(img)\n",
    "\n",
    "\n",
    "imageio.mimsave('/data/wenhao/20241027_humanoid/Safe-Policy-Optimization/safepo/collected_data/single_agent_exp/SafetyHumanoidVelocity-v1/ppo_lag/seed-000-2024-11-03-01-43-36/track/track004.gif', images)\n",
    "\n",
    "\n",
    "\n",
    "images = []\n",
    "\n",
    "# get all files in the replay folder\n",
    "files = os.listdir(fixednear)\n",
    "files = [f for f in files if f.endswith(\".png\")]\n",
    "\n",
    "for i in range(len(files)):\n",
    "    # Load the image\n",
    "    if i < 10:\n",
    "        img = imageio.imread(fixednear + \"0\" + str(i) + \".png\")\n",
    "    else:\n",
    "        img = imageio.imread(fixednear + str(i) + \".png\")\n",
    "    \n",
    "    # # Rotate the image by 180 degrees\n",
    "    # img_rotated = np.rot90(img, 2)  # Rotates 180 degrees (90 degrees twice)\n",
    "\n",
    "    # # Append the rotated image to the list\n",
    "    # images.append(img_rotated)\n",
    "    \n",
    "    images.append(img)\n",
    "    \n",
    "imageio.mimsave('/data/wenhao/20241027_humanoid/Safe-Policy-Optimization/safepo/collected_data/single_agent_exp/SafetyHumanoidVelocity-v1/ppo_lag/seed-000-2024-11-03-01-43-36/fixednear/fixednear004.gif', images)\n",
    "\n",
    "# images = []\n",
    "\n",
    "# # get all files in the replay folder\n",
    "# files = os.listdir(corner3)\n",
    "# files = [f for f in files if f.endswith(\".png\")]\n",
    "\n",
    "# for i in range(len(files)):\n",
    "#     # Load the image\n",
    "#     if i < 10:\n",
    "#         img = imageio.imread(corner3 + \"0\" + str(i) + \".png\")\n",
    "#     else:\n",
    "#         img = imageio.imread(corner3 + str(i) + \".png\")\n",
    "    \n",
    "#     # Rotate the image by 180 degrees\n",
    "#     img_rotated = np.rot90(img, 2)  # Rotates 180 degrees (90 degrees twice)\n",
    "\n",
    "#     # Append the rotated image to the list\n",
    "#     images.append(img_rotated)\n",
    "\n",
    "\n",
    "# imageio.mimsave('/data/wenhao/20241027_humanoid/metaworld_dataset_1112_test/door-open/corner3.gif', images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[4.9480795691705035, 4.83886256315303, 4.927295934802406, 4.860377869841213, 4.781098108893878, 4.89238530768223, 4.906880739448507, 4.847839807110334, 4.8879599535447795, 4.920643892277639, 4.962294699567341, 4.859139668765443, 4.911462941119612, 4.927955890438026, 4.946550835413266, 4.901479688729054, 4.8980908226111195, 4.962152785141824, 5.012623108272416, 5.030113166190967, 4.886022885973135, 4.942607384585707, 4.954509497475873, 4.980477400350476, 4.960653476701247, 4.948225370920107, 4.982060395829901, 4.9767160137957855, 4.976155565200187, 4.951779161121912, 4.961211328088977, 4.960708853234926, 4.944685107422648, 4.925682082052916, 4.994235999688695, 4.957851607077792, 4.933042587842145, 4.995632438678534, 4.949849310605122, 4.935483680545076, 4.986917590283291, 4.945570893583034, 4.970055299191465, 4.949678875098255, 5.004351797128589, 4.947278221005582, 4.971878379645822, 4.964430841160263, 4.988516170432319, 5.012644681109165, 4.9094119224444315, 4.961146824514079, 4.9577223886127095, 4.982938386991778, 5.006255727136978, 4.882690210204156, 4.9492768750062, 4.993767808300805, 4.984767767558739, 4.856650062125971, 4.969834686152488, 4.9707727664842745, 5.0545330351790465, 4.980178533188254, 4.974408037558972, 5.005270907859182, 4.990910087705906, 4.9522552233803125, 4.936895953188635, 4.983053363580217, 5.00480153627597, 5.005916641637739, 4.926418469954484, 4.954557677323882, 5.008981769908074, 5.0140691603505445, 4.952158183988646, 4.994122294468673, 5.0420501115940874, 5.059679183660154, 5.0235885813321435, 5.054243321512964, 5.081304654318575, 5.079307810503566, 5.070293062805004, 5.081586787278424, 5.067158568715116, 5.066401159318589, 5.072095016106668, 5.063856447030946, 5.091170887950829, 5.146811716648965, 5.119651941463522, 5.066729678554018, 5.097881133383235, 5.150337642732112, 5.179172584503516, 5.092098114277649, 5.105089746698747, 5.149358186206871, 5.192576110204119, 5.140892253577391, 5.137612569070036, 5.139466928631003, 5.173488144131577, 5.189605685833122, 5.189979230176118, 5.234480766297342, 5.207450605207808, 5.220544241246073, 5.264240434677693, 5.2591737858976995, 5.260343935053742, 5.223456627981159, 5.216512722120036, 5.221560182001079, 5.228090041615812, 5.233042824884135, 5.237422106863615, 5.242149036133903, 5.244174901737215, 5.249683548713227, 5.250409922253485, 5.226419975689735, 5.192595443798223, 5.189452039543053, 5.211029210243565, 5.232305538537789, 5.2533881173657, 5.279050842704994, 5.308429122555806, 5.3290178171293565, 5.334047146211038, 5.324956467508046, 5.326559333915353, 5.327768308444734, 5.326899827319274, 5.341907209893789, 5.500038581618286, 5.597559139424808, 5.6415179306835, 5.672095284437604, 5.70591456565104, 5.727661124168476, 5.750349614753682, 5.795547719000638, 5.84839719415247, 5.901897991769103, 5.952069903698938]\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open('/data/wenhao/20241027_humanoid/Safe-Policy-Optimization/safepo/collected_data/single_agent_exp/SafetyHumanoidVelocity-v1/ppo_lag/seed-000-2024-11-03-01-43-36/fixednear/004/costs.pkl', 'rb') as f:\n",
    "    costs = pkl.load(f)\n",
    "    \n",
    "# rewards \n",
    "with open('/data/wenhao/20241027_humanoid/Safe-Policy-Optimization/safepo/collected_data/single_agent_exp/SafetyHumanoidVelocity-v1/ppo_lag/seed-000-2024-11-03-01-43-36/fixednear/004/rewards.pkl', 'rb') as f:\n",
    "    rewards = pkl.load(f)\n",
    "    \n",
    "print(costs)\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaworld.envs import ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE as env_dict\n",
    "import metaworld.policies as policies\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import imageio\n",
    "import os\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    collection_config = {\n",
    "        \"demos_per_camera\": 25,\n",
    "        \"output_path\": \"./collected_data\",\n",
    "        \"camera_names\": ['corner', 'corner2', 'corner3'], ### possible values: \"corner3, corner, corner2, topview, behindGripper\", None(for random)\n",
    "        \"discard_ratio\": 0.0, ### discard the last {ratio} of the collected videos (preventing failed episodes)\n",
    "        \"render_mode\": \"rgb_array\"\n",
    "    }\n",
    "\n",
    "\n",
    "    included_tasks = [\"door-open\"]# [\"drawer-open\", \"door-close\", \"basketball\", \"shelf-place\", \"button-press\", \"button-press-topdown\", \"faucet-close\", \"faucet-open\", \"handle-press\", \"hammer\", \"assembly\"]\n",
    "    included_tasks = [t + \"-v2-goal-observable\" for t in included_tasks]\n",
    "    #included_tasks = ['assembly-v2-goal-observable', 'basketball-v2-goal-observable', 'bin-picking-v2-goal-observable', 'box-close-v2-goal-observable', 'button-press-topdown-v2-goal-observable', 'button-press-topdown-wall-v2-goal-observable', 'button-press-v2-goal-observable', 'button-press-wall-v2-goal-observable', 'coffee-button-v2-goal-observable', 'coffee-pull-v2-goal-observable', 'coffee-push-v2-goal-observable', 'dial-turn-v2-goal-observable', 'disassemble-v2-goal-observable', 'door-close-v2-goal-observable', 'door-lock-v2-goal-observable', 'door-open-v2-goal-observable', 'door-unlock-v2-goal-observable', 'hand-insert-v2-goal-observable', 'drawer-close-v2-goal-observable', 'drawer-open-v2-goal-observable', 'faucet-open-v2-goal-observable', 'faucet-close-v2-goal-observable', 'hammer-v2-goal-observable', 'handle-press-side-v2-goal-observable', 'handle-press-v2-goal-observable', 'handle-pull-side-v2-goal-observable', 'handle-pull-v2-goal-observable', 'lever-pull-v2-goal-observable', 'pick-place-wall-v2-goal-observable', 'pick-out-of-hole-v2-goal-observable', 'reach-v2-goal-observable', 'push-back-v2-goal-observable', 'push-v2-goal-observable', 'pick-place-v2-goal-observable', 'plate-slide-v2-goal-observable', 'plate-slide-side-v2-goal-observable', 'plate-slide-back-v2-goal-observable', 'plate-slide-back-side-v2-goal-observable', 'peg-unplug-side-v2-goal-observable', 'soccer-v2-goal-observable', 'stick-push-v2-goal-observable', 'stick-pull-v2-goal-observable', 'push-wall-v2-goal-observable', 'reach-wall-v2-goal-observable', 'shelf-place-v2-goal-observable', 'sweep-into-v2-goal-observable', 'sweep-v2-goal-observable', 'window-open-v2-goal-observable', 'window-close-v2-goal-observable']\n",
    "    \n",
    "    \n",
    "    def get_policy(env_name):\n",
    "        name = \"\".join(\" \".join(env_name.split('-')[:-3]).title().split(\" \"))\n",
    "        policy_name = \"Sawyer\" + name + \"V2Policy\"\n",
    "        try:\n",
    "            policy = getattr(policies, policy_name)()\n",
    "        except:\n",
    "            policy = None\n",
    "        return policy\n",
    "\n",
    "    def save_frame(path, frame):\n",
    "        imageio.imwrite(path, frame)\n",
    "        \n",
    "    ps = {}\n",
    "    for env_name in env_dict.keys():\n",
    "        policy = get_policy(env_name)\n",
    "        if policy is None:\n",
    "            print(\"Policy not found:\", env_name)\n",
    "        else:\n",
    "            ps[env_name] = policy\n",
    "\n",
    "    out_path = collection_config[\"output_path\"]\n",
    "\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    for task in tqdm(included_tasks):\n",
    "        print(task)\n",
    "        for camera in collection_config[\"camera_names\"]:\n",
    "            demos = []\n",
    "            action_seqs = []\n",
    "            raw_lengths = []\n",
    "            rewards = []\n",
    "            for seed in tqdm(range(42, 42+ceil(collection_config[\"demos_per_camera\"] * (1+collection_config[\"discard_ratio\"])))):\n",
    "                env = env_dict[task](render_mode=collection_config[\"render_mode\"], camera_name=camera, seed=seed)\n",
    "                obs = env.reset()\n",
    "                images, action_seq, reward, _ = collect_video(obs, env, ps[task])\n",
    "                # assert len(images) == len(action_seq) + 1 or len(images) == 502\n",
    "                raw_lengths += [len(images)]\n",
    "                demos += [images]\n",
    "                action_seqs += [action_seq]\n",
    "                rewards += [reward]\n",
    "            top_k_ind = np.argsort(raw_lengths)[:collection_config[\"demos_per_camera\"]]\n",
    "            demos = [demos[i] for i in top_k_ind]\n",
    "            raw_lengths = [raw_lengths[i] for i in top_k_ind]\n",
    "            action_seqs = [action_seqs[i] for i in top_k_ind]\n",
    "            rewards = [rewards[i] for i in top_k_ind]\n",
    "            print(f\"vid length bounds: {raw_lengths[0]} ~ {raw_lengths[-1]}\")\n",
    "            \n",
    "            ### save the collected demos\n",
    "            out_dir = os.path.join(out_path, \"-\".join(task.split('-')[:-3]))\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            for i, demo in enumerate(demos):\n",
    "                demo_dir = os.path.join(out_dir, f\"{camera}/{i:03d}\")\n",
    "                os.makedirs(demo_dir, exist_ok=True)\n",
    "                with mp.Pool(10) as p:\n",
    "                    p.starmap(save_frame, [(os.path.join(demo_dir, f\"{j:02d}.png\"), frame) for j, frame in enumerate(demo)])\n",
    "                with open(f\"{demo_dir}/action.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(action_seqs[i], f)\n",
    "                with open(f\"{demo_dir}/rewards.pkl\", \"wb\") as f:\n",
    "                        pickle.dump(rewards[i], f)\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 OmniSafeAI Team. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "from collections import deque\n",
    "from safepo.common.env import make_sa_mujoco_env, make_ma_mujoco_env, make_ma_multi_goal_env, make_ma_isaac_env\n",
    "from safepo.common.model import ActorVCritic\n",
    "from safepo.utils.config import multi_agent_velocity_map, multi_agent_goal_tasks, isaac_gym_map\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "def eval_single_agent(eval_dir, eval_episodes):\n",
    "\n",
    "    torch.set_num_threads(4)\n",
    "    config_path = eval_dir + '/config.json'\n",
    "    config = json.load(open(config_path, 'r'))\n",
    "\n",
    "    env_id = config['task'] if 'task' in config.keys() else config['env_name']\n",
    "    env_norms = os.listdir(eval_dir)\n",
    "    env_norms = [env_norm for env_norm in env_norms if env_norm.endswith('.pkl')]\n",
    "    final_norm_name = sorted(env_norms)[-1]\n",
    "\n",
    "    model_dir = eval_dir + '/torch_save'\n",
    "    models = os.listdir(model_dir)\n",
    "    models = [model for model in models if model.endswith('.pt')]\n",
    "    final_model_name = sorted(models)[-1]\n",
    "\n",
    "    model_path = model_dir + '/' + final_model_name\n",
    "    norm_path = eval_dir + '/' + final_norm_name\n",
    "\n",
    "    eval_env, obs_space, act_space = make_sa_mujoco_env(num_envs=config['num_envs'], env_id=env_id, seed=None)\n",
    "\n",
    "    model = ActorVCritic(\n",
    "            obs_dim=obs_space.shape[0],\n",
    "            act_dim=act_space.shape[0],\n",
    "            hidden_sizes=config['hidden_sizes'],\n",
    "        )\n",
    "    model.actor.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    if os.path.exists(norm_path):\n",
    "        norm = joblib.load(open(norm_path, 'rb'))['Normalizer']\n",
    "        eval_env.obs_rms = norm\n",
    "\n",
    "    eval_rew_deque = deque(maxlen=50)\n",
    "    eval_cost_deque = deque(maxlen=50)\n",
    "    eval_len_deque = deque(maxlen=50)\n",
    "\n",
    "    for _ in range(eval_episodes):\n",
    "        eval_done = False\n",
    "        eval_obs, _ = eval_env.reset()\n",
    "        eval_obs = torch.as_tensor(eval_obs, dtype=torch.float32)\n",
    "        eval_rew, eval_cost, eval_len = 0.0, 0.0, 0.0\n",
    "        while not eval_done:\n",
    "            with torch.no_grad():\n",
    "                act, _, _, _ = model.step(\n",
    "                    eval_obs, deterministic=True\n",
    "                )\n",
    "            eval_obs, reward, cost, terminated, truncated, info = eval_env.step(\n",
    "                act.detach().squeeze().cpu().numpy()\n",
    "            )\n",
    "            eval_obs = torch.as_tensor(\n",
    "                eval_obs, dtype=torch.float32\n",
    "            )\n",
    "            eval_rew += reward[0]\n",
    "            eval_cost += cost[0]\n",
    "            eval_len += 1\n",
    "            eval_done = terminated[0] or truncated[0]\n",
    "        eval_rew_deque.append(eval_rew)\n",
    "        eval_cost_deque.append(eval_cost)\n",
    "        eval_len_deque.append(eval_len)\n",
    "\n",
    "    return sum(eval_rew_deque) / len(eval_rew_deque), sum(eval_cost_deque) / len(eval_cost_deque)\n",
    "\n",
    "\n",
    "def eval_multi_agent(eval_dir, eval_episodes):\n",
    "\n",
    "    config_path = eval_dir + '/config.json'\n",
    "    config = json.load(open(config_path, 'r'))\n",
    "\n",
    "    env_name = config['env_name']\n",
    "    if env_name in multi_agent_velocity_map.keys():\n",
    "        env_info = multi_agent_velocity_map[env_name]\n",
    "        agent_conf = env_info['agent_conf']\n",
    "        scenario = env_info['scenario']\n",
    "        eval_env = make_ma_mujoco_env(\n",
    "            scenario=scenario,\n",
    "            agent_conf=agent_conf,\n",
    "            seed=np.random.randint(0, 1000),\n",
    "            cfg_train=config,\n",
    "        )\n",
    "    # elif env_name in isaac_gym_map:\n",
    "    #     import pdb; pdb.set_trace()\n",
    "    #     cfg_train=config\n",
    "    #     agent_index = [[[0, 1, 2, 3, 4, 5]],\n",
    "    #                 [[0, 1, 2, 3, 4, 5]]]\n",
    "    #     raise NotImplementedError \n",
    "    #     # sim_params = parse_sim_params(args, cfg_env, cfg_train)\n",
    "    #     # env = make_ma_isaac_env(args, cfg_env, cfg_train, sim_params, agent_index)\n",
    "        \n",
    "    #     # cfg_train[\"n_rollout_threads\"] = env.num_envs\n",
    "    #     # cfg_train[\"n_eval_rollout_threads\"] = env.num_envs\n",
    "    #     # eval_env = make_ma_isaac_env(\n",
    "    #     #     task=env_name,\n",
    "    #     #     seed=np.random.randint(0, 1000),\n",
    "    #     #     cfg_train=config,\n",
    "    #     # )\n",
    "    else:\n",
    "        eval_env = make_ma_multi_goal_env(\n",
    "            task=env_name,\n",
    "            seed=np.random.randint(0, 1000),\n",
    "            cfg_train=config,\n",
    "        )\n",
    "\n",
    "    model_dir = eval_dir + f\"/models_seed{config['seed']}\"\n",
    "    algo = config['algorithm_name']\n",
    "    if algo == 'macpo':\n",
    "        from safepo.multi_agent.macpo import Runner\n",
    "    elif algo == 'mappo':\n",
    "        from safepo.multi_agent.mappo import Runner\n",
    "    elif algo == 'mappolag':\n",
    "        from safepo.multi_agent.mappolag import Runner\n",
    "    elif algo == 'happo':\n",
    "        from safepo.multi_agent.happo import Runner\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    torch.set_num_threads(4)\n",
    "    runner = Runner(\n",
    "        vec_env=eval_env,\n",
    "        vec_eval_env=eval_env,\n",
    "        config=config,\n",
    "        model_dir=model_dir,\n",
    "    )\n",
    "    return runner.eval(eval_episodes)\n",
    "\n",
    "\n",
    "def single_runs_eval(eval_dir, eval_episodes):\n",
    "\n",
    "    config_path = eval_dir + '/config.json'\n",
    "    config = json.load(open(config_path, 'r'))\n",
    "    env = config['task'] if 'task' in config.keys() else config['env_name']\n",
    "    if env in multi_agent_velocity_map.keys() or env in multi_agent_goal_tasks or env in isaac_gym_map:\n",
    "        reward, cost = eval_multi_agent(eval_dir, eval_episodes)\n",
    "    else:\n",
    "        reward, cost = eval_single_agent(eval_dir, eval_episodes)\n",
    "    \n",
    "    return reward, cost\n",
    "\n",
    "def benchmark_eval():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--benchmark-dir\", type=str, default='', help=\"the directory of the evaluation\")\n",
    "    parser.add_argument(\"--eval-episodes\", type=int, default=3, help=\"the number of episodes to evaluate\")\n",
    "    parser.add_argument(\"--save-dir\", type=str, default=None, help=\"the directory to save the evaluation result\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    benchmark_dir = args.benchmark_dir\n",
    "    eval_episodes = args.eval_episodes\n",
    "    if args.save_dir is not None:\n",
    "        save_dir = args.save_dir\n",
    "    else:\n",
    "        save_dir = benchmark_dir.replace('runs', 'results')\n",
    "        if os.path.exists(save_dir) is False:\n",
    "            os.makedirs(save_dir)\n",
    "    envs = os.listdir(benchmark_dir)\n",
    "    for env in envs:\n",
    "        env_path = os.path.join(benchmark_dir, env)\n",
    "        algos = os.listdir(env_path)\n",
    "        for algo in algos:\n",
    "            print(f\"Start evaluating {algo} in {env}\")\n",
    "            algo_path = os.path.join(env_path, algo)\n",
    "            seeds = os.listdir(algo_path)\n",
    "            rewards, costs = [], []\n",
    "            for seed in seeds:\n",
    "                seed_path = os.path.join(algo_path, seed)\n",
    "                reward, cost = single_runs_eval(seed_path, eval_episodes)\n",
    "                rewards.append(reward)\n",
    "                costs.append(cost)\n",
    "            output_file = open(f\"{save_dir}/eval_result.txt\", 'a')\n",
    "            # two wise after point\n",
    "            reward_mean = round(np.mean(rewards), 2)\n",
    "            reward_std = round(np.std(rewards), 2)\n",
    "            cost_mean = round(np.mean(costs), 2)\n",
    "            cost_std = round(np.std(costs), 2)\n",
    "            print(f\"After {eval_episodes} episodes evaluation, the {algo} in {env} evaluation reward: {reward_mean}±{reward_std}, cost: {cost_mean}±{cost_std}, the reuslt is saved in {save_dir}/eval_result.txt\")\n",
    "            output_file.write(f\"After {eval_episodes} episodes evaluation, the {algo} in {env} evaluation reward: {reward_mean}±{reward_std}, cost: {cost_mean}±{cost_std} \\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    benchmark_eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
